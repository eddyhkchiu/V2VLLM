<div align="center">

# V2V-LLM: Vehicle-to-Vehicle Cooperative Autonomous Driving with Multi-Modal Large Language Models

Hsu-kuang Chiu<sup>1,2</sup>, Ryo Hachiuma<sup>1</sup>, Chien-Yi Wang<sup>1</sup>, Stephen F. Smith<sup>2</sup>, Yu-Chiang Frank Wang<sup>1</sup>, Min-Hung Chen<sup>1</sup>

<sup>1</sup>NVIDIA Research, <sup>2</sup>Carnegie Mellon University



[project](https://eddyhkchiu.github.io/v2vllm.github.io/)  [arxiv](https://arxiv.org/abs/2502.09980)

<img src="images/conceptual_only_1113_MHC.jpg" height=400px>

</div>

## Overview

We propose a novel problem setting that integrates an LLM into cooperative autonomous driving, with the proposed <b>Vehicle-to-Vehicle Question-Answering (V2V-QA)</b> dataset and benchmark. We also propose our baseline method <b>Vehicle-to-Vehicle Large Language Model (V2V-LLM)</b>, which uses an LLM to fuse perception information from multiple connected autonomous vehicles (CAVs) and answer driving-related questions: grounding, notable object identification, and planning. Experimental results show that our proposed V2V-LLM can be a promising unified model architecture for performing various tasks in cooperative autonomous driving, and outperforms other baseline methods that use different fusion approaches. Our work also creates a new research direction that can improve the safety of future autonomous driving systems. For more details, please refer to our paper at <a href="https://arxiv.org/abs/2502.09980">arxiv</a>.

